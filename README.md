# Distributed-Key-Value-Database
### High Level Approach

Our program implements the RAFT protocol that allows the distributive system maintaining a consistent database system. The distributive database system consists of several replicas that store the information. The program allows the client to send "put" and "get" command to every replica and make sure those commands are successfully processed and stored in each replica. The RAFT protocol is a 2PC state check system that has one leader and the other replicas being followers. The leader is responsible for processing and committing the client's request. The leader is chosen by election. Election is trigger by a random timeout (between 150ms - 300 ms) on each replica. If a replica reaches the timeout, it will promote itself as the candidate and then send out requestVote message to each replica. When a replica has not yet voted, it will respond with a True message to tell the leader that it receives the vote request and vote for the sending candidate. Until the new leader is selected, the follower can only vote for one leader (if two followers start timeout at similar time and there is no enough time for electing a new leader). When the leader received more than half of the replicas' votes, it will become the leader and broadcast to everyone. If a partition appears and there are two followers, these two followers will not get into an election since the population for the group is too low to start an election. Leader will send out a heartbeat information to each replica and replica uses this message to know whether the leader is alive or not. When a client sends a message to any replica, if the receiver is not the leader, it will forward the message to the leader and marked as redirected and then the leader will broadcast the client's command. Each replica will operate this command and send a message as "OK" to the leader. After receiving majority (more than half) of followers' "OK" messages, the leader will send a "OK" to the client claiming that the operation is successfully conducted. Each new leader will have its own term number that uses this number to synchronize the logs between different replicas and old leaders. LARGER term number will always override the log a replica has that has a lower term number. Also, LastLogIndex takes part into the decision of overriding which log and using which log (larger number always wins). With this mechanism, the distributed database system will make sure every replica shares the same data and being consistent after each operation from the clients. If a replica is in a partition, it will record all the commands from the client it received and forward them to the leader after the partition disappears. Both the leader and follower will have a temporary appendEntry list that uses for detecting the crash of each replica or the leader. Also, the temporary list will hold the client's request that is waiting for the "OK" message from the majority of the replicas. 

### Challenges:

The election of leaders from follower is a challenge. This is because you will expect many different situation that a replica will met when time out happens and those edge cases will influence the result of the election. 
The append entry RPC is another challenge and the main difficulty for it is to maintain a consistency among all replicas on the logs' entries. The leader needs to know when should I update logs of its own entry, and whether or not the entry is successfully commit to every replica's entry, and what should it do if some replica doesn't have some index of entry and many situations needs also be captured. At last, debugging in this project is very hard since the log are thousands of lines of redirecting (also heartbeat), getting and putting messages and the reasons of causing a failure are various. Since each testing is based on random tests, the result will differ a lot and the errors appear will also vary basing the different input variables. 

### Properties/features of design:

When a leader is killed given by accident, one of the remaining follower will timeout accordingly and begin the election. It will check for the votes it receive including the term and log index number and compare with term and log index of itselves. If it founds out that it is not the most up-to-date replica among all followers, it will return to follower from candidate state and wait until others time_out and vote for them.
When a new put operation is demanded by the client, the leader will receive this message either redirected by other replicas, or directly from user. Then it will remember this operation with its value and ask every replica to append this key value pair into their own logs. If over half of the replicas succeeded in appending the entry, then it will reply to user with ok message and it appends this value into its own log. For the remaining replicas that haven't yet responded, it will retry the same message to that replica until it either respond with an success message( in this case network is delayed) or after a few times trying with no response, then it believe the replica is killed. 

### Testing:

We have ran all the tests given in the config file. We have went through each part of the code to do unit test such that they work independently. We have use message printing technique such that useful message is printed during each test cases.
